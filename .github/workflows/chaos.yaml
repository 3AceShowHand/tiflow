name: chaos

on:
  push:
    branches:
      - master
      - 'release-[0-9].[0-9]*'
  pull_request:
    branches:
      - master
      - 'release-[0-9].[0-9]*'
  schedule:
    - cron: '0,30 17-22 * * *' # run at minute 0 and 30 every hour from 01:00 ~ 06:00 UTC+8

jobs:
  pre_job:
    runs-on: ubuntu-latest
    timeout-minutes: 40
    outputs:
      should_skip: ${{ steps.skip_check.outputs.should_skip }}
    steps:
      - id: skip_check
        uses: fkirc/skip-duplicate-actions@master
        with:
          # All of these options are optional, so you can remove them if you are happy with the defaults
          # https://github.com/marketplace/actions/skip-duplicate-actions
          concurrent_skipping: 'never'
          skip_after_successful_duplicate: 'true'
          paths_ignore: '["**/README.md"]'
          cancel_others: 'true'
          do_not_skip: '["workflow_dispatch", "schedule"]' # only skip pull_request

  # This workflow contains a single job called "main_job"
  main_job:
    needs: pre_job
    if: ${{ needs.pre_job.outputs.should_skip != 'true' }}
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    timeout-minutes: 40
    strategy:
      fail-fast: false
      matrix:
        chaos-obj: ["pod-failure-cdc", "pod-kill-cdc", "network-partition-cdc", "network-emulation-cdc", "io-chaos-cdc"]

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: Set up Go
        uses: actions/setup-go@v2
        with:
          go-version: '1.16.4'
      - name: Print Go version
        run: go version

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Check out code
        uses: actions/checkout@v2

      - name: Cache Lint Tools
        id: cache-lint-tools
        uses: actions/cache@v2
        with:
          path: tools/bin
          key: ${{ runner.os }}-cdc-lint-tools-${{ hashFiles('tools/check/go.sum') }}

      # Set up Kubernetes with K3s
      - name: Set up K3s cluster
        run: |
          curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.18.9+k3s1 sh -s - \
            --write-kubeconfig-mode=644 \
            "${k3s_disable_command:---disable}" metrics-server \
            "${k3s_disable_command:---disable}" traefik \
            --flannel-backend=none \
            --docker
        shell: bash

      # this may be failed sometimes, and I want to exit the workflow directly if failed,
      # but GitHub Actions doesnt' support early-exit yet, see https://github.com/actions/runner/issues/662.
      # so, simply wait for a long time.
      - name: Wait for coredns
        run: |
          kubectl rollout status --watch --timeout 600s deployment/coredns -n kube-system
        shell: bash
        env:
          KUBECONFIG: /etc/rancher/k3s/k3s.yaml

      - name: Export KUBECONFIG environment variable
        run: |
          echo 'KUBECONFIG=/etc/rancher/k3s/k3s.yaml' >> $GITHUB_ENV
          kubectl cluster-info
        shell: bash

      - name: Print cluster information
        run: |
          kubectl config view
          kubectl cluster-info
          kubectl get nodes
          kubectl get pods -n kube-system
          kubectl get sc
          kubectl version

      - name: install tidb-operator
        run: |
          echo "install helm 3"
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

          echo "install tidb-operator CRDs"
          kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/v1.1.12/manifests/crd.yaml

          echo "install tidb-operator into namespace playground"
          helm repo add pingcap https://charts.pingcap.org/
          kubectl create namespace playground
          helm install --namespace playground tidb-operator pingcap/tidb-operator --version v1.1.12

          kubectl get pods --namespace playground -l app.kubernetes.io/instance=tidb-operator

      - name: Deploy upstream TiDB Cluster into playground
        run: |
          kubectl appy -f $GITHUB_WORKSPACE/chaos/manifests/upstream.yaml -n playground
          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/upstream.yaml -n playground
          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/upstream.yaml -n playground

      - name: Deploy downstream TiDB Cluster into playground
        run: |
          kubectl appy -f $GITHUB_WORKSPACE/chaos/manifests/downstream.yaml -n playground
          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/downstream.yaml -n playground
          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/downstream.yaml -n playground

      - name: Deploy CDC into playground
        run: |
          docker build -t cdc $GITHUB_WORKSPACE
          docker image list
          kubectl apply -f $GITHUB_WORKSPACE/chaos/manifests/ticdc.yaml -n playground
          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/ticdc.yaml -n playground
          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/ticdc.yaml -n playground

      - name: Deploy bank workload into playground
        run: |
          docker build -f $GITHUB_WORKSPACE/chaos/Dockerfile-chaos -t bank-workload $GITHUB_WORKSPACE
          docker image list

          kubectl apply -f $GITHUB_WORKSPACE/chaos/manifests/workload.yaml -n playground
          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/workload.yaml -n playground
          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/workload.yaml -n playground

      - name: Wait for sources ready # kubectl wait --all not working
        run: |
          kubectl wait --for=condition=Ready pod/mysql57-0 --timeout=300s || true
          kubectl wait --for=condition=Ready pod/mysql8-0 --timeout=300s || true
          kubectl wait --for=condition=Ready pod/mariadb-0 --timeout=300s || true
          sleep 10
          echo show pvc
          kubectl get pvc -l app=sources -o wide
          echo show pv
          kubectl get pv -o wide
          echo show svc
          kubectl get svc -l app=sources -o wide
          echo show sts
          kubectl get sts -l app=sources -o wide
          echo show po
          kubectl get po -l app=sources -o wide
          echo describe po
          kubectl describe po -l app=sources
          echo describe pvc
          kubectl describe pvc -l app=sources
          kubectl wait --for=condition=Ready pod/mysql57-0 --timeout=0s
          kubectl wait --for=condition=Ready pod/mysql8-0 --timeout=0s
          kubectl wait --for=condition=Ready pod/mariadb-0 --timeout=0s

      # Set up downstream TiDB instance (deploy a TiDB with mockTiKV, not a TidbCluster managed by TiDB-operator)
      - name: Set up TiDB
        run: |
          kubectl apply -f $GITHUB_WORKSPACE/chaos/manifests/tidb.yaml
          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/tidb.yaml
          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/tidb.yaml

      - name: Wait for TiDB ready
        run: |
          kubectl wait --for=condition=Ready pod/tidb-0 --timeout=300s || true
          echo show pvc
          kubectl get pvc -l app=tidb -o wide
          echo show pv
          kubectl get pv -o wide
          echo show svc
          kubectl get svc -l app=tidb -o wide
          echo show sts
          kubectl get sts -l app=tidb -o wide
          echo show po
          kubectl get po -l app=tidb -o wide
          echo describe po
          kubectl describe po -l app=tidb
          echo describe pvc
          kubectl describe pvc -l app=tidb
          kubectl wait --for=condition=Ready pod/tidb-0 --timeout=0s

#      - name: Set up DM-master
#        run: |
#          kubectl apply -f $GITHUB_WORKSPACE/chaos/manifests/dm-master.yaml
#          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/dm-master.yaml
#          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/dm-master.yaml

      # NOTE: even some DM-master instances are not ready, we still continue and let chaos test cases to check again.
#      - name: Wait for DM-master ready
#        run: |
#          sleep 10
#          kubectl wait --for=condition=Ready pod -l app=dm-master --all --timeout=300s || true
#          echo "<<<<< show pvc >>>>>"
#          kubectl get pvc -l app=dm-master -o wide
#          echo "<<<<< show pv >>>>>"
#          kubectl get pv -o wide
#          echo "<<<<< show svc >>>>>"
#          kubectl get svc -l app=dm-master -o wide
#          echo "<<<<< show sts >>>>>"
#          kubectl get sts -l app=dm-master -o wide
#          echo "<<<<< show po >>>>>"
#          kubectl get po -l app=dm-master -o wide
#          echo "<<<<< describe po >>>>>"
#          kubectl describe po -l app=dm-master
#          echo "<<<<< describe pvc >>>>>"
#          kubectl describe pvc -l app=dm-master
#          echo "<<<<< show current log for dm-master-0 >>>>>"
#          kubectl logs dm-master-0 || true
#          echo "<<<<< show previous log for dm-master-0 >>>>>"
#          kubectl logs dm-master-0 -p || true
#          echo "<<<<< show current log for dm-master-1 >>>>>"
#          kubectl logs dm-master-1 || true
#          echo "<<<<< show previous log for dm-master-1 >>>>>"
#          kubectl logs dm-master-1 -p || true
#          echo "<<<<< show current log for dm-master-2 >>>>>"
#          kubectl logs dm-master-2 || true
#          echo "<<<<< show previous log for dm-master-2 >>>>>"
#          kubectl logs dm-master-2 -p || true

#      - name: Set up DM-worker
#        run: |
#          kubectl apply -f $GITHUB_WORKSPACE/chaos/manifests/dm-worker.yaml
#          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/dm-worker.yaml
#          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/dm-worker.yaml
#      # NOTE: even some DM-worker instances are not ready, we still continue and let chaos test cases to check again.
#
#      - name: Wait for DM-worker ready
#        run: |
#          sleep 10
#          kubectl wait --for=condition=Ready pod -l app=dm-worker --all --timeout=300s || true
#          echo "<<<<< show pvc >>>>>"
#          kubectl get pvc -l app=dm-worker -o wide
#          echo "<<<<< show pv >>>>>"
#          kubectl get pv -o wide
#          echo "<<<<< show svc >>>>>"
#          kubectl get svc -l app=dm-worker -o wide
#          echo "<<<<< show sts >>>>>"
#          kubectl get sts -l app=dm-worker -o wide
#          echo "<<<<< show po >>>>>"
#          kubectl get po -l app=dm-worker -o wide
#          echo "<<<<< describe po >>>>>"
#          kubectl describe po -l app=dm-worker
#          echo "<<<<< describe pvc >>>>>"
#          kubectl describe pvc -l app=dm-worker
#          echo "<<<<< show current log for dm-worker-0 >>>>>"
#          kubectl logs dm-worker-0 || true
#          echo "<<<<< show previous log for dm-worker-0 >>>>>"
#          kubectl logs dm-worker-0 -p || true
#          echo "<<<<< show current log for dm-worker-1 >>>>>"
#          kubectl logs dm-worker-1 || true
#          echo "<<<<< show previous log for worker-master-1 >>>>>"
#          kubectl logs dm-worker-1 -p || true
#          echo "<<<<< show current log for dm-worker-2 >>>>>"
#          kubectl logs dm-worker-2 || true
#          echo "<<<<< show previous log for dm-worker-2 >>>>>"
#          kubectl logs dm-worker-2 -p || true
      # NOTE: we sleep a while when check members ready in cases before applying any chaos operations.

      - name: Set up chaos test cases
        run: |
          kubectl apply -f $GITHUB_WORKSPACE/chaos/manifests/cases.yaml
          kubectl get -f $GITHUB_WORKSPACE/chaos/manifests/cases.yaml
          kubectl describe -f $GITHUB_WORKSPACE/chaos/manifests/cases.yaml
          sleep 60

      - name: Encode chaos-mesh action
        run: |
          echo CFG_BASE64=$(base64 -w 0 $GITHUB_WORKSPACE/chaos/manifests/${{ matrix.chaos-obj }}.yaml) >> $GITHUB_ENV

      - name: Run chaos mesh action
        uses: chaos-mesh/chaos-mesh-action@master
        env:
          CFG_BASE64: ${{ env.CFG_BASE64 }}

      # check whether complete with 1m * 20 times.
      - name: Wait for chaos test case complete
        run: |
          $GITHUB_WORKSPACE/chaos/scripts/check-case.sh

      - name: Copy logs to hack permission
        if: ${{ always() }}
        run: |
          mkdir ./logs
          sudo cp -r -L /var/log/containers/. ./logs
          sudo find /var/ -type f -regex '.*/dm-[^/]*.log$' | sudo xargs -i cp {} ./logs || true
          sudo chown -R runner ./logs

      # Update logs as artifact seems not stable, so we set `continue-on-error: true` here.
      - name: Upload logs
        continue-on-error: true
        uses: actions/upload-artifact@v2
        if: ${{ always() }}
        with:
          name: chaos-base-logs.${{ matrix.chaos-obj }}
          path: |
            ./logs
            !./logs/coredns-*
            !./logs/local-path-provisioner-*
